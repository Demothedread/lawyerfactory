# Script Name: fact_objectivity_agent.py
# Description: Fact Objectivity Agent for LawyerFactory Orchestration Phase.  This agent ensures that facts are presented objectively in legal documents while maintaining a favorable perspective for the client. It performs:  - Fact verification and validation - Objectivity assessment and adjustment - Favorable framing of facts - Elimination of cherry-picking - Consistency checking across documents  The agent ensures that while facts must be accurate and complete, they are presented in a manner most advantageous to the client's case.
# Relationships:
#   - Entity Type: Agent
#   - Directory Group: Orchestration
#   - Group Tags: null
Fact Objectivity Agent for LawyerFactory Orchestration Phase.

This agent ensures that facts are presented objectively in legal documents
while maintaining a favorable perspective for the client. It performs:

- Fact verification and validation
- Objectivity assessment and adjustment
- Favorable framing of facts
- Elimination of cherry-picking
- Consistency checking across documents

The agent ensures that while facts must be accurate and complete,
they are presented in a manner most advantageous to the client's case.
"""

import logging
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass, field

from ...compose.maestro.registry import AgentInterface, AgentCapability
from ...compose.maestro.workflow_models import WorkflowTask

logger = logging.getLogger(__name__)


@dataclass
class FactAssessment:
    """Assessment of a fact's objectivity and framing"""
    fact_text: str
    objectivity_score: float  # 0.0 to 1.0
    framing_issues: List[str] = field(default_factory=list)
    suggested_reframing: Optional[str] = None
    source_verification: bool = False
    completeness_check: bool = True


class FactObjectivityAgent(AgentInterface):
    """Agent that ensures factual objectivity while maintaining favorable presentation"""

    def __init__(self, knowledge_graph=None):
        self.knowledge_graph = knowledge_graph
        self.capabilities = [AgentCapability.LEGAL_RESEARCH, AgentCapability.CASE_ANALYSIS]

        # Load objectivity guidelines and patterns
        self._load_objectivity_guidelines()

    def _load_objectivity_guidelines(self):
        """Load guidelines for factual objectivity"""
        self.objectivity_patterns = {
            "biased_language": [
                "clearly", "obviously", "undoubtedly", "without question",
                "obviously", "clearly", "definitely", "certainly"
            ],
            "conclusionary_statements": [
                "was negligent", "breached the contract", "committed fraud",
                "was careless", "acted intentionally"
            ],
            "unsupported_claims": [
                "knew or should have known", "failed to properly",
                "negligent behavior", "reckless actions"
            ],
            "favorable_adjectives": [
                "reasonable", "prudent", "careful", "professional",
                "competent", "qualified", "experienced"
            ]
        }

        self.objectivity_guidelines = {
            "completeness": "All facts necessary to support claims must be included",
            "accuracy": "Facts must be accurate and verifiable",
            "relevance": "Only facts relevant to claims should be included",
            "objectivity": "Present facts objectively, not as conclusions",
            "favorable_framing": "Frame facts favorably to client while maintaining truth"
        }

    async def process(self, message: str) -> str:
        """Process a natural language request for fact objectivity analysis"""
        try:
            # Extract facts from the message
            facts = self._extract_facts_from_text(message)

            # Analyze fact objectivity
            assessments = await self.analyze_fact_objectivity(facts)

            # Generate response
            response = "Fact Objectivity Analysis:\n\n"
            for i, assessment in enumerate(assessments, 1):
                response += f"{i}. **Fact:** {assessment.fact_text[:100]}...\n"
                response += f"   Objectivity Score: {assessment.objectivity_score:.2f}/1.0\n"

                if assessment.framing_issues:
                    response += f"   Issues: {', '.join(assessment.framing_issues)}\n"

                if assessment.suggested_reframing:
                    response += f"   Suggested: {assessment.suggested_reframing}\n"

                response += "\n"

            return response

        except Exception as e:
            logger.error(f"Error processing fact objectivity request: {e}")
            return f"Error analyzing fact objectivity: {str(e)}"

    async def execute_task(self, task: WorkflowTask, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a workflow task related to fact objectivity"""
        try:
            # Extract facts from context
            facts = self._extract_facts_from_context(context)

            # Analyze objectivity
            assessments = await self.analyze_fact_objectivity(facts)

            # Generate improved version
            improved_facts = self._generate_improved_facts(assessments)

            # Check for missing facts
            missing_facts = await self.identify_missing_facts(facts, context)

            return {
                "status": "completed",
                "facts_analyzed": len(facts),
                "assessments": [assessment.__dict__ for assessment in assessments],
                "improved_facts": improved_facts,
                "missing_facts": missing_facts,
                "overall_objectivity_score": self._calculate_overall_score(assessments)
            }

        except Exception as e:
            logger.error(f"Error executing fact objectivity task: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "facts_analyzed": 0,
                "assessments": [],
                "improved_facts": [],
                "missing_facts": [],
                "overall_objectivity_score": 0.0
            }

    async def health_check(self) -> bool:
        """Check if the agent is functioning properly"""
        try:
            # Test basic functionality
            test_facts = ["The defendant was driving recklessly"]
            assessments = await self.analyze_fact_objectivity(test_facts)
            return len(assessments) > 0
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False

    async def initialize(self) -> None:
        """Initialize the agent"""
        try:
            logger.info("Fact Objectivity Agent initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Fact Objectivity Agent: {e}")

    async def cleanup(self) -> None:
        """Clean up resources"""
        pass

    async def can_handle_task(self, task: WorkflowTask) -> bool:
        """Check if this agent can handle the given task"""
        task_text = f"{task.description} {task.agent_type}".lower()
        return any(keyword in task_text for keyword in [
            "fact", "objectivity", "bias", "framing", "statement"
        ])

    async def analyze_fact_objectivity(self, facts: List[str]) -> List[FactAssessment]:
        """Analyze the objectivity of given facts"""
        assessments = []

        for fact in facts:
            assessment = await self._assess_single_fact(fact)
            assessments.append(assessment)

        return assessments

    async def identify_missing_facts(self, existing_facts: List[str], context: Dict[str, Any]) -> List[str]:
        """Identify facts that should be included but are missing"""
        missing_facts = []

        # Get claims from context
        claims = context.get("claims_matrix", {}).get("claims", [])
        if not claims:
            return missing_facts

        # Analyze each claim for required facts
        for claim in claims:
            if isinstance(claim, dict):
                claim_title = claim.get("title", "")
                required_elements = self._get_required_fact_elements(claim_title)

                # Check if facts supporting these elements exist
                for element in required_elements:
                    if not self._fact_exists_for_element(element, existing_facts):
                        missing_facts.append(f"Missing fact for {element} in {claim_title}")

        return missing_facts

    def _extract_facts_from_text(self, text: str) -> List[str]:
        """Extract facts from natural language text"""
        # Simple sentence-based extraction
        sentences = text.replace('?', '.').replace('!', '.').split('.')
        facts = [s.strip() for s in sentences if len(s.strip()) > 10]
        return facts

    def _extract_facts_from_context(self, context: Dict[str, Any]) -> List[str]:
        """Extract facts from workflow context"""
        facts = []

        # Look for facts in various context fields
        if "facts" in context:
            facts.extend(context["facts"])
        if "evidence_table" in context:
            for evidence in context["evidence_table"]:
                if isinstance(evidence, dict):
                    facts.append(evidence.get("title", str(evidence)))
                else:
                    facts.append(str(evidence))
        if "fact_matrix" in context:
            fact_matrix = context["fact_matrix"]
            if isinstance(fact_matrix, dict):
                undisputed_facts = fact_matrix.get("undisputed_facts", [])
                facts.extend(undisputed_facts)

        return list(set(facts))  # Remove duplicates

    async def _assess_single_fact(self, fact: str) -> FactAssessment:
        """Assess the objectivity of a single fact"""
        issues = []
        score = 1.0  # Start with perfect score
        suggested_reframing = None

        fact_lower = fact.lower()

        # Check for biased language
        for word in self.objectivity_patterns["biased_language"]:
            if word in fact_lower:
                issues.append(f"Contains biased language: '{word}'")
                score -= 0.1

        # Check for conclusionary statements
        for phrase in self.objectivity_patterns["conclusionary_statements"]:
            if phrase in fact_lower:
                issues.append(f"Conclusionary statement: '{phrase}'")
                score -= 0.2
                suggested_reframing = self._generate_neutral_alternative(fact, phrase)

        # Check for unsupported claims
        for phrase in self.objectivity_patterns["unsupported_claims"]:
            if phrase in fact_lower:
                issues.append(f"Unsupported characterization: '{phrase}'")
                score -= 0.15

        # Ensure score doesn't go below 0
        score = max(0.0, score)

        # Check completeness
        completeness = self._assess_fact_completeness(fact)
        if not completeness:
            issues.append("May be incomplete or missing context")
            score -= 0.1

        return FactAssessment(
            fact_text=fact,
            objectivity_score=score,
            framing_issues=issues,
            suggested_reframing=suggested_reframing,
            source_verification=self._can_verify_source(fact),
            completeness_check=completeness
        )

    def _generate_neutral_alternative(self, fact: str, biased_phrase: str) -> str:
        """Generate a more neutral alternative to a biased phrase"""
        replacements = {
            "was negligent": "failed to exercise reasonable care",
            "breached the contract": "did not fulfill contractual obligations",
            "committed fraud": "made material misrepresentations",
            "was careless": "did not act with appropriate caution",
            "acted intentionally": "acted with specific purpose"
        }

        for biased, neutral in replacements.items():
            if biased in fact.lower():
                return fact.replace(biased, neutral)

        return fact

    def _assess_fact_completeness(self, fact: str) -> bool:
        """Assess if a fact is complete and provides necessary context"""
        # Check for key elements: who, what, when, where, why, how
        has_who = any(word in fact.lower() for word in ["plaintiff", "defendant", "party", "person", "company"])
        has_what = len(fact.split()) > 3  # Basic check for sufficient detail
        has_when = any(word in fact.lower() for word in ["on", "date", "time", "during", "after", "before"])

        # Fact is considered complete if it has at least 2 of 3 key elements
        return sum([has_who, has_what, has_when]) >= 2

    def _can_verify_source(self, fact: str) -> bool:
        """Check if the fact's source can be verified"""
        # Look for verifiable elements like dates, names, specific details
        has_date = any(word.isdigit() and len(word) == 4 for word in fact.split())
        has_specific_name = any(word.istitle() and len(word) > 3 for word in fact.split())
        has_specific_detail = len(fact.split()) > 5

        return has_date or (has_specific_name and has_specific_detail)

    def _get_required_fact_elements(self, claim_title: str) -> List[str]:
        """Get the factual elements required to support a claim"""
        elements_map = {
            "negligence": ["duty", "breach", "causation", "damages"],
            "breach of contract": ["offer", "acceptance", "consideration", "breach", "damages"],
            "fraud": ["misrepresentation", "materiality", "reliance", "damages"],
            "products liability": ["defect", "causation", "damages"]
        }

        claim_lower = claim_title.lower()
        for key, elements in elements_map.items():
            if key in claim_lower:
                return elements

        return ["relevant facts", "causation", "damages"]  # Default elements

    def _fact_exists_for_element(self, element: str, facts: List[str]) -> bool:
        """Check if a fact exists that supports a required element"""
        element_keywords = {
            "duty": ["duty", "responsible", "obligation", "care"],
            "breach": ["breach", "failed", "violate", "not comply"],
            "causation": ["caused", "resulted", "led to", "because"],
            "damages": ["damages", "harm", "injury", "loss", "cost"],
            "offer": ["offer", "proposal", "agreement"],
            "acceptance": ["accept", "agree", "consent"],
            "consideration": ["payment", "value", "exchange"]
        }

        keywords = element_keywords.get(element.lower(), [element.lower()])

        for fact in facts:
            fact_lower = fact.lower()
            if any(keyword in fact_lower for keyword in keywords):
                return True

        return False

    def _generate_improved_facts(self, assessments: List[FactAssessment]) -> List[str]:
        """Generate improved versions of facts based on assessments"""
        improved_facts = []

        for assessment in assessments:
            if assessment.objectivity_score < 0.8 and assessment.suggested_reframing:
                improved_facts.append(assessment.suggested_reframing)
            else:
                improved_facts.append(assessment.fact_text)

        return improved_facts

    def _calculate_overall_score(self, assessments: List[FactAssessment]) -> float:
        """Calculate overall objectivity score"""
        if not assessments:
            return 1.0

        total_score = sum(assessment.objectivity_score for assessment in assessments)
        return total_score / len(assessments)